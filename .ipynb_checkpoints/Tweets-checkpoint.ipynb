{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570300767074181121</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:33 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>570300616901320704</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cjmcginnis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:13:57 -0800</td>\n",
       "      <td>San Francisco CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>570300248553349120</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pilot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:12:29 -0800</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>570299953286942721</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dhepburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:11:19 -0800</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>570295459631263746</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YupitsTate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:53:27 -0800</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>570294189143031808</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>idk_but_youtube</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica did you know that suicide is th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:48:24 -0800</td>\n",
       "      <td>1/1 loner squad</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>570289724453216256</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HyperCamiLax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:30:40 -0800</td>\n",
       "      <td>NYC</td>\n",
       "      <td>America/New_York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>570289584061480960</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HyperCamiLax</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica This is such a great deal! Alre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:30:06 -0800</td>\n",
       "      <td>NYC</td>\n",
       "      <td>America/New_York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>570287408438120448</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mollanderson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:21:28 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>570285904809598977</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sjespers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica Thanks!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:15:29 -0800</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0   570306133677760513           neutral                        1.0000   \n",
       "1   570301130888122368          positive                        0.3486   \n",
       "2   570301083672813571           neutral                        0.6837   \n",
       "3   570301031407624196          negative                        1.0000   \n",
       "4   570300817074462722          negative                        1.0000   \n",
       "5   570300767074181121          negative                        1.0000   \n",
       "6   570300616901320704          positive                        0.6745   \n",
       "7   570300248553349120           neutral                        0.6340   \n",
       "8   570299953286942721          positive                        0.6559   \n",
       "9   570295459631263746          positive                        1.0000   \n",
       "10  570294189143031808           neutral                        0.6769   \n",
       "11  570289724453216256          positive                        1.0000   \n",
       "12  570289584061480960          positive                        1.0000   \n",
       "13  570287408438120448          positive                        0.6451   \n",
       "14  570285904809598977          positive                        1.0000   \n",
       "\n",
       "   negativereason  negativereason_confidence         airline  \\\n",
       "0             NaN                        NaN  Virgin America   \n",
       "1             NaN                     0.0000  Virgin America   \n",
       "2             NaN                        NaN  Virgin America   \n",
       "3      Bad Flight                     0.7033  Virgin America   \n",
       "4      Can't Tell                     1.0000  Virgin America   \n",
       "5      Can't Tell                     0.6842  Virgin America   \n",
       "6             NaN                     0.0000  Virgin America   \n",
       "7             NaN                        NaN  Virgin America   \n",
       "8             NaN                        NaN  Virgin America   \n",
       "9             NaN                        NaN  Virgin America   \n",
       "10            NaN                     0.0000  Virgin America   \n",
       "11            NaN                        NaN  Virgin America   \n",
       "12            NaN                        NaN  Virgin America   \n",
       "13            NaN                        NaN  Virgin America   \n",
       "14            NaN                        NaN  Virgin America   \n",
       "\n",
       "   airline_sentiment_gold             name negativereason_gold  retweet_count  \\\n",
       "0                     NaN          cairdin                 NaN              0   \n",
       "1                     NaN         jnardino                 NaN              0   \n",
       "2                     NaN       yvonnalynn                 NaN              0   \n",
       "3                     NaN         jnardino                 NaN              0   \n",
       "4                     NaN         jnardino                 NaN              0   \n",
       "5                     NaN         jnardino                 NaN              0   \n",
       "6                     NaN       cjmcginnis                 NaN              0   \n",
       "7                     NaN            pilot                 NaN              0   \n",
       "8                     NaN         dhepburn                 NaN              0   \n",
       "9                     NaN       YupitsTate                 NaN              0   \n",
       "10                    NaN  idk_but_youtube                 NaN              0   \n",
       "11                    NaN     HyperCamiLax                 NaN              0   \n",
       "12                    NaN     HyperCamiLax                 NaN              0   \n",
       "13                    NaN     mollanderson                 NaN              0   \n",
       "14                    NaN         sjespers                 NaN              0   \n",
       "\n",
       "                                                 text tweet_coord  \\\n",
       "0                 @VirginAmerica What @dhepburn said.         NaN   \n",
       "1   @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2   @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3   @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4   @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "5   @VirginAmerica seriously would pay $30 a fligh...         NaN   \n",
       "6   @VirginAmerica yes, nearly every time I fly VX...         NaN   \n",
       "7   @VirginAmerica Really missed a prime opportuni...         NaN   \n",
       "8     @virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D         NaN   \n",
       "9   @VirginAmerica it was amazing, and arrived an ...         NaN   \n",
       "10  @VirginAmerica did you know that suicide is th...         NaN   \n",
       "11  @VirginAmerica I &lt;3 pretty graphics. so muc...         NaN   \n",
       "12  @VirginAmerica This is such a great deal! Alre...         NaN   \n",
       "13  @VirginAmerica @virginmedia I'm flying your #f...         NaN   \n",
       "14                             @VirginAmerica Thanks!         NaN   \n",
       "\n",
       "                tweet_created     tweet_location               user_timezone  \n",
       "0   2015-02-24 11:35:52 -0800                NaN  Eastern Time (US & Canada)  \n",
       "1   2015-02-24 11:15:59 -0800                NaN  Pacific Time (US & Canada)  \n",
       "2   2015-02-24 11:15:48 -0800          Lets Play  Central Time (US & Canada)  \n",
       "3   2015-02-24 11:15:36 -0800                NaN  Pacific Time (US & Canada)  \n",
       "4   2015-02-24 11:14:45 -0800                NaN  Pacific Time (US & Canada)  \n",
       "5   2015-02-24 11:14:33 -0800                NaN  Pacific Time (US & Canada)  \n",
       "6   2015-02-24 11:13:57 -0800   San Francisco CA  Pacific Time (US & Canada)  \n",
       "7   2015-02-24 11:12:29 -0800        Los Angeles  Pacific Time (US & Canada)  \n",
       "8   2015-02-24 11:11:19 -0800          San Diego  Pacific Time (US & Canada)  \n",
       "9   2015-02-24 10:53:27 -0800        Los Angeles  Eastern Time (US & Canada)  \n",
       "10  2015-02-24 10:48:24 -0800    1/1 loner squad  Eastern Time (US & Canada)  \n",
       "11  2015-02-24 10:30:40 -0800                NYC            America/New_York  \n",
       "12  2015-02-24 10:30:06 -0800                NYC            America/New_York  \n",
       "13  2015-02-24 10:21:28 -0800                NaN  Eastern Time (US & Canada)  \n",
       "14  2015-02-24 10:15:29 -0800  San Francisco, CA  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 tweet_id                        14402\n",
      "airline_sentiment               14402\n",
      "airline_sentiment_confidence    14402\n",
      "negativereason                   9113\n",
      "negativereason_confidence       10284\n",
      "airline                         14402\n",
      "airline_sentiment_gold             40\n",
      "name                            14402\n",
      "negativereason_gold                32\n",
      "retweet_count                   14402\n",
      "text                            14402\n",
      "tweet_coord                      1009\n",
      "tweet_created                   14402\n",
      "tweet_location                   9749\n",
      "user_timezone                    9657\n",
      "dtype: int64\n",
      "0.7 tweet_id                        10760\n",
      "airline_sentiment               10760\n",
      "airline_sentiment_confidence    10760\n",
      "negativereason                   7546\n",
      "negativereason_confidence        7629\n",
      "airline                         10760\n",
      "airline_sentiment_gold             39\n",
      "name                            10760\n",
      "negativereason_gold                32\n",
      "retweet_count                   10760\n",
      "text                            10760\n",
      "tweet_coord                       766\n",
      "tweet_created                   10760\n",
      "tweet_location                   7212\n",
      "user_timezone                    7154\n",
      "dtype: int64\n",
      "0.8 tweet_id                        10459\n",
      "airline_sentiment               10459\n",
      "airline_sentiment_confidence    10459\n",
      "negativereason                   7392\n",
      "negativereason_confidence        7396\n",
      "airline                         10459\n",
      "airline_sentiment_gold             37\n",
      "name                            10459\n",
      "negativereason_gold                30\n",
      "retweet_count                   10459\n",
      "text                            10459\n",
      "tweet_coord                       746\n",
      "tweet_created                   10459\n",
      "tweet_location                   7007\n",
      "user_timezone                    6960\n",
      "dtype: int64\n",
      "0.9 tweet_id                        10458\n",
      "airline_sentiment               10458\n",
      "airline_sentiment_confidence    10458\n",
      "negativereason                   7391\n",
      "negativereason_confidence        7395\n",
      "airline                         10458\n",
      "airline_sentiment_gold             36\n",
      "name                            10458\n",
      "negativereason_gold                29\n",
      "retweet_count                   10458\n",
      "text                            10458\n",
      "tweet_coord                       745\n",
      "tweet_created                   10458\n",
      "tweet_location                   7006\n",
      "user_timezone                    6959\n",
      "dtype: int64\n",
      "1.0 tweet_id                        10445\n",
      "airline_sentiment               10445\n",
      "airline_sentiment_confidence    10445\n",
      "negativereason                   7382\n",
      "negativereason_confidence        7382\n",
      "airline                         10445\n",
      "airline_sentiment_gold             23\n",
      "name                            10445\n",
      "negativereason_gold                20\n",
      "retweet_count                   10445\n",
      "text                            10445\n",
      "tweet_coord                       745\n",
      "tweet_created                   10445\n",
      "tweet_location                   6995\n",
      "user_timezone                    6948\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('0.6', df.where(df['airline_sentiment_confidence'] > 0.6).count())\n",
    "print('0.7', df.where(df['airline_sentiment_confidence'] > 0.7).count())\n",
    "print('0.8', df.where(df['airline_sentiment_confidence'] > 0.8).count())\n",
    "print('0.9', df.where(df['airline_sentiment_confidence'] > 0.9).count())\n",
    "print('1.0', df.where(df['airline_sentiment_confidence'] == 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                        10445\n",
       "airline_sentiment               10445\n",
       "airline_sentiment_confidence    10445\n",
       "negativereason                   7382\n",
       "negativereason_confidence        7382\n",
       "airline                         10445\n",
       "airline_sentiment_gold             23\n",
       "name                            10445\n",
       "negativereason_gold                20\n",
       "retweet_count                   10445\n",
       "text                            10445\n",
       "tweet_coord                       745\n",
       "tweet_created                   10445\n",
       "tweet_location                   6995\n",
       "user_timezone                    6948\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df['airline_sentiment_confidence'] == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  What @dhepburn said.\n",
      "  What   said.\n",
      "  plus you've added commercials to the experience... tacky.\n",
      "  I didn't today... Must mean I need to take another trip!\n",
      "  it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "  and it's a really big bad thing about it\n",
      "  seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      "  yes, nearly every time I fly VX this ‚Äúear worm‚Äù won‚Äôt go away :)\n",
      "  Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\n",
      "  Well, I didn't‚Ä¶but NOW I DO! :-D\n",
      "  it was amazing, and arrived an hour early. You're too good to me.\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "count = 0\n",
    "count_lines = 0\n",
    "for i in texts:\n",
    "    #result = re.match('[@A-Z,a-z]*', i)\n",
    "    #while result is not None:\n",
    "    all_x = re.findall('[@][a-z]*', i, re.I)\n",
    "    if all_x is not None:\n",
    "        count_lines += 1\n",
    "    #if len(all_x) > 1:\n",
    "        #print(i)\n",
    "    for x in all_x:\n",
    "        i = i.replace(x, ' ')\n",
    "    #    result = re.match('[@A-Z,a-z]*', i)\n",
    "        print(i)\n",
    "    # else:\n",
    "        count += 1\n",
    "    '''print(result, i)\n",
    "    count += 1'''\n",
    "    if count > 10:\n",
    "        break\n",
    "    # i = i.replace(result, ' ')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14640 1943\n"
     ]
    }
   ],
   "source": [
    "print(count_lines, count - count_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.@SouthwestAir Thx for the follow up. Just sent DM',\n",
       " '.@SouthwestAir she needs a wheelchair at the gate for sure. What options does she have?',\n",
       " '1/2 @united Just be honest with your customers. The new seats are designed to fit more rows on the plane. More rows = more money.',\n",
       " '@AmericanAir  please call us back to rebook!!! 7403607771. We need to get back to Columbus!!!!!! Please help',\n",
       " '@AmericanAir \"Thank you for contacting American. The email address you have written to is an unmonitored account‚Äù',\n",
       " '@AmericanAir #AmericanView Sweet Home Chicago http://t.co/J6icLV8DTs',\n",
       " '@AmericanAir ...have you seen Blue? Go look it up :)',\n",
       " \"@AmericanAir 1491 DEN -&gt; DFW. We don't want to be ticketed on a new flight, just refunded for our Cancelled Flighted flight.\",\n",
       " '@AmericanAir 3611 was supposed to depart at 5:10pm central.  About ready to close the door at 7:34pm central.',\n",
       " '@AmericanAir @AmericanAir how do you expect to do that?',\n",
       " '@AmericanAir @karabuxthomps Yeah, I have a lot of questions, number one: how dare you',\n",
       " '@AmericanAir @lpalumbo what weather sun is out',\n",
       " '@AmericanAir @maxfitgirl29 next time fly Southwest...',\n",
       " \"@AmericanAir @yvonneokaka When do I get my personal response and apology for your crew's having forgotten to load baggage onto my flight?\",\n",
       " '@AmericanAir AA45 ... JFK to LAS.',\n",
       " '@AmericanAir Amazing to watch @chasefoster (who travels with celebs all over the world) show the youth of America who *not* to fly with.',\n",
       " '@AmericanAir But you called me - and hung up.',\n",
       " \"@AmericanAir Can you add my KTN to an existing reservation? It's not letting me add it online....\",\n",
       " \"@AmericanAir Hey AA - can you help with an itinerary for a plat custy?  Stuck in PVR and phones aren't working\",\n",
       " '@AmericanAir I already landed! But, if you still care it was 1891. Thanks for the help though!, do you k ow what caused the failure?',\n",
       " '@AmericanAir I am looking for help on USAirways award travel booked for wife and two boys, no seats assigned.',\n",
       " '@AmericanAir I am trying to switch my flight to AA 1359 I am currently on AA 2401 at 6:50am MONDAY morn then AA 2586! Help Me!!',\n",
       " \"@AmericanAir I don't think you should help him at all based on his behavior. The voucher and cot seem like enough lol üòÉ\",\n",
       " \"@AmericanAir I have a ticket that has been pending for 4 days. Is it normal to take that long?? I want to make sure there isn't a problem.\",\n",
       " '@AmericanAir I look forward to hearing back and hope that my next trip will not be like this',\n",
       " '@AmericanAir I loved todays fight. But there was a bad draft on the airplane please add more air to air thanksm',\n",
       " '@AmericanAir I need assistance reFlight Booking Problems a flight from dfw to lit',\n",
       " '@AmericanAir I need help Cancelled Flighting an upcoming flight made with AAdvantage miles. Record locator HFJKTO. Can someone please assist?',\n",
       " '@AmericanAir I need refund.',\n",
       " \"@AmericanAir I understand that but I'm hoping I can rectify this in advance. I would call but I am in AUH. I just need my one checked bag.\",\n",
       " \"@AmericanAir I will thank you. What do I do if I can't get through and my flight on hold Cancelled Flights?\",\n",
       " '@AmericanAir I wrote down the evoucher number. I got rid of the voucher when I purchased air fair',\n",
       " \"@AmericanAir I'm flying into DCA my bag is at IAD.  I am already Late Flight for my meetings at work.  I will call the number when I land üò≠\",\n",
       " \"@AmericanAir I'm not sure what happened to my USAirways status when the merger took place.\",\n",
       " '@AmericanAir Just followed you.',\n",
       " '@AmericanAir Kudos to ticket agents for #2224 for making passengers check bags that are too big to fit in overhead.',\n",
       " '@AmericanAir Thanks. Having issues checking in for flight, please check our DM convo for more info.',\n",
       " '@AmericanAir Was not on board you today just watched report unfold on Twitter but still am very proud!',\n",
       " \"@AmericanAir What's with your baggage handlers calling out at DFW today? No luggage onboard the planes.\",\n",
       " '@AmericanAir Will it be this year, do you think?',\n",
       " \"@AmericanAir a free check bag a food comp a flying credit something to help out now that I'm an additional 200 dollars in the hole\",\n",
       " '@AmericanAir after all, the plane didn‚Äôt land in identical or worse) conditions at GRK according to METARs.',\n",
       " '@AmericanAir although there was a 6 hour delay every single staff member from the ticket desk to the admirals club in was as sweet as pie',\n",
       " \"@AmericanAir but seriously if my cats dead I'm going to be pissed/very thankful because it is evil and wants my soul\",\n",
       " '@AmericanAir daughter record loc. is KYBILC. She is traveling with 3 y.o. and needs to be in adjacent seats, not across isle.',\n",
       " \"@AmericanAir don't see response!\",\n",
       " '@AmericanAir flight was 2488 out of EWR STOP AT DALLAS THEN TO LA. I need to be in la tonight!',\n",
       " '@AmericanAir how are the runways at DFW? Will you be flying Late Flightr today? Thanks.',\n",
       " '@AmericanAir im still on hold...',\n",
       " '@AmericanAir is there anyone online that can help... the flight leaves in 3 hrs',\n",
       " \"@AmericanAir no kidding! Gonna take some beating on the apron... And there are some good lookin' planes out there!\",\n",
       " '@AmericanAir not anymore.',\n",
       " '@AmericanAir overbooked business class from Mia to uvf - after being in the correct seat we were moved back to economy on our special day',\n",
       " '@AmericanAir so whats the hold up on flight 4302?',\n",
       " '@AmericanAir thanks',\n",
       " '@AmericanAir thanks for keeping us safe',\n",
       " '@AmericanAir thanks, but I had to hire a car in Chicago and drive to MKE. Does that mean I have to go to the airport?????',\n",
       " \"@AmericanAir the status on my itinerary says on request in red. What's that mean?\",\n",
       " '@AmericanAir what are my chances of making a connection to El Paso (AA504) with DFW from SAT (AA200) delayed 30 minutes?',\n",
       " \"@AmericanAir what's the status of flight 3494 XNA to DFW. I'm getting different reports?\",\n",
       " \"@AmericanAir when will tomorrow's flight Cancelled Flightlations at Dfw for AA flights be posted? We are on 2424 at 7am from LAX!\",\n",
       " \"@AmericanAir wow that's helpful.\",\n",
       " \"@AmericanAir. Had my flight changed from US Airways to you, now I can't get two seats together. Can you help?\",\n",
       " \"@DeltaAssist what I have to say is more than 140 characters! Plus you don't follow me\",\n",
       " '@JetBlue #flyfi thank you! Seattle and #UDUB here we come! @cameron__roe http://t.co/kbHYM5GkAP',\n",
       " '@JetBlue - looking forward to it when we finally take off.',\n",
       " '@JetBlue A320 pulling into the gate as the sunrises here at @BostonLogan this morning #jetbluesofly #jetblue #airbus http://t.co/JGdu5us8Dz',\n",
       " '@JetBlue Airways Sees Significant Decline in Short Interest (JBLU) - WKRB News http://t.co/lZFGUTxIYN',\n",
       " '@JetBlue Been dealing with a Cancelled Flightled flight since last night &amp; was booked to a 6:15 flight,wouldve at least liked to see Ella perform lol',\n",
       " '@JetBlue Flight I want to book was $320 one day; went to purchase next day &amp; price doubled to $737. Inquiring on chance it may go down?',\n",
       " \"@JetBlue If your confirming to me that the rate won't go down again looks like I can't fly JetBlue this time around, can't afford that price\",\n",
       " '@JetBlue Just got the time to reply and share my thoughts in greater detail via web form.',\n",
       " '@JetBlue Now ur asking for the heavy guns! You know #IWouldDoAnythingForLove https://t.co/Yx1DQJn8nL #BrandMance #LoveSongFriday',\n",
       " '@JetBlue Oh no! I thought it did. :(',\n",
       " \"@JetBlue That'd be nice! Hoping to rack up enough miles to take a trip to Seattle and enjoy a perfect latte in the city of coffee.\",\n",
       " \"@JetBlue The 13th Annual Martha's Vineyard African American Film Festival  August 10-15, 2015   Our attendees deserve a great flight to MV??\",\n",
       " '@JetBlue actually flight 1089 not 1098',\n",
       " \"@JetBlue and stranded passengers aren't being helped to get another flight?\",\n",
       " '@JetBlue claims w/ customer protection they \"will notify customers of delays, Cancelled Flightlations and diversions.\" #NotTRUE http://t.co/3oq1T4ohjl',\n",
       " '@JetBlue does transferring trueblue points within family pooling members cost $? This is after having set up contribution-to-pool already.',\n",
       " \"@JetBlue figured it out 4 flight 2morrow. Hard to believe with oil so low, and the killing u make that even part of a hotel can't be comp'd\",\n",
       " '@JetBlue got it. Thanks',\n",
       " '@JetBlue he loved the #natural #beefjerky snacksüòâ',\n",
       " '@JetBlue hope so! looks nice and warm in San Juan üå¥',\n",
       " '@JetBlue if I tell you I like that @warriors pick, will that get me that bump to first class? :)',\n",
       " \"@JetBlue if I'm about to get on a plane, I think I deserve to know what the mechanical issues are for the flight being delayed.\",\n",
       " '@JetBlue if you can get us on a flight to Vieques all will be good. Right now, not looking promising.',\n",
       " \"@JetBlue if you want to fly in a storm that's your right. But give us the choice. We have kids and don't want to chance it. #nixchangefees\",\n",
       " '@JetBlue my email is my twitter handle followed by gmail ‚ò∫üëçüëç',\n",
       " \"@JetBlue no, but we're on the flight leaving from Boston to Seattle right now. :) flight 597\",\n",
       " '@JetBlue please provide me your direct email for me to explain.',\n",
       " '@JetBlue sadly, no! I have the app, but it also is experiencing difficulties. The flight information boards are keeping me updated.',\n",
       " '@JetBlue sooo earlier i said i couldnt fly with you for my school trip but now i can! üòè',\n",
       " '@JetBlue thanks!',\n",
       " \"@JetBlue thanks! I'll do it.\",\n",
       " '@JetBlue the gate agent said our original plane got de-icing fluid in the engine that created a short so they took the plane out of service',\n",
       " '@JetBlue the pilot just stated that after 22 years of flying he has never experienced anything like this. #gettingoffplane',\n",
       " '@JetBlue time to reevaluate my nyc carrier.',\n",
       " '@JetBlue to Increase Charter Service to Cuba - #Travel Agent http://t.co/lYQrb4HCYU',\n",
       " \"@JetBlue toss this ticket...it's great PR and I'm sure every college student following me will be willing to rock out wit too üëÄüëÄ\",\n",
       " \"@JetBlue traveling w/ a rifle for a hunting trip, can a pistol be in the same large rifle case too? Wasn't clear in freq asked questions\",\n",
       " \"@JetBlue unveils new 'Bluemanity' livery - USA TODAY http://t.co/ueGgKN79zA\",\n",
       " '@JetBlue well lucky I only fly JB..I guess I would be even more squished on any other airline üòû',\n",
       " '@JetBlue what is the reason for the delay',\n",
       " '@JetBlue when can we start Flight Booking Problems flights for November?',\n",
       " '@JetBlue when your flights delayed :)))&gt;&gt;&gt;',\n",
       " \"@JetBlue y'all got prices for the low??\",\n",
       " \"@JetBlue yeah I'm aware of that. Hence my original msg to you.\",\n",
       " '@JetBlue yes. They are working on it. Hoping bag is on the next flight and will deliver home...',\n",
       " \"@NinaDavuluri We think it's a treat to have you onboard! Enjoy your flight. üíô\",\n",
       " '@SouthwestAir  Please Help. We really want to fly with you. #guessweflyingdelta',\n",
       " '@SouthwestAir  Thank you.',\n",
       " '@SouthwestAir -U dont have Atlanta to San fransisco in 99$ in the present sale, like u had in the last sale?Im looking for 2 return tickets',\n",
       " \"@SouthwestAir @Imaginedragons when are we gonna know I have a math test tomoro and I can't concentrateüò≠üò≠ #DestinationDragons\",\n",
       " '@SouthwestAir @LukeWyckoff: her name is Jane and she wanted was a call. 617-653-3040',\n",
       " \"@SouthwestAir After flying Delta this week, GoGo was so good. The tweet I sent you didn't even send until we landed - that's how bad it was\",\n",
       " '@SouthwestAir Can a pair of tickets waiting for me after my sports practice? I live here and definitely know where that is! @Imaginedragons',\n",
       " \"@SouthwestAir How's Dallas incoming looking? I'm scheduled to fly in at 10:30 and wondering if I'll be stuck in St. Louis instead.\",\n",
       " \"@SouthwestAir I already follow you and can't DM because u need to follow me back\",\n",
       " \"@SouthwestAir I am trying to book another flight and apply my credits that I had from another flight but I can't find them.\",\n",
       " \"@SouthwestAir I did. Still haven't heard a thing.\",\n",
       " '@SouthwestAir I love imagine dragons o flipping much, pls #DestinationDragons',\n",
       " \"@SouthwestAir I understand. But it's consistent, and been consistent for years. It's known around the country.\",\n",
       " '@SouthwestAir Just got companion pass and trying to add companion flgt but get purchase.error.INVALID_LOYALTY_MEMBER_ACCOUNT_STATUS. Help!',\n",
       " '@SouthwestAir Kudos to bag handler at DTW gate 21 at 7:10 pm. Ran to keep bags from hitting ground in 9 degree temp! http://t.co/5tXU5tSFkJ',\n",
       " '@SouthwestAir Please bring back RDU to FLL direct route!  I noticed it is missing starting in Late Flight August!  The flights are always packed!',\n",
       " '@SouthwestAir Salted or honey roasted? I vote to bring back the salted peanuts. I dread a year of the honey roasted!üòñ http://t.co/RHw78ktQFO',\n",
       " \"@SouthwestAir Thank you.  Twitter says I can't DM someone unless they follow me.  Can @SouthwestAir follows my twitter?\",\n",
       " '@SouthwestAir Thanks! I know the routes you currently have from Newark. I was just wondering if there was any new ones on the horizon',\n",
       " '@SouthwestAir When does the new schedule open that takes us past 10/31?',\n",
       " '@SouthwestAir Yo, I\\'m trying to make a change to a reservation but whenever I select the flight it says, \"Unable to price\". Please help!',\n",
       " '@SouthwestAir alas, it was pretty full so we had to entertain ourselves with witty repartee ...',\n",
       " '@SouthwestAir any chance I can get tix for @VelourLive @Imaginedragons? I have been tweeting since January 26 about it #DestinationDragons',\n",
       " '@SouthwestAir any delays today @CAKairport to Orlando?',\n",
       " '@SouthwestAir any idea on flights to Nashville tomorrow being clear or Cancelled Flightled',\n",
       " '@SouthwestAir any word when the customer relations department will open back up?',\n",
       " '@SouthwestAir could care more about their customer, if there is a reasonable request :(',\n",
       " '@SouthwestAir filing it now. Thank you for your response.',\n",
       " '@SouthwestAir following. Thank you.',\n",
       " '@SouthwestAir fyi the link in your baggage incident email redirects to a 404 on mobile. The link auto changes on mobile. Desktop works fine.',\n",
       " \"@SouthwestAir got an email confirmation of wifi purchase on a recent flight. Trouble is, I've never flown with you. Ever. #concerned #scam?\",\n",
       " \"@SouthwestAir how do I find travel funds I have available to use f/ past changed flights? don't see an option in my account.\",\n",
       " '@SouthwestAir is service to Aruba being permanently discontinued in August?',\n",
       " \"@SouthwestAir it's not letting me DM you !!\",\n",
       " '@SouthwestAir never mind, I moved my flight to tomorrow. Thanks for the help!',\n",
       " \"@SouthwestAir nvm, gate said they're holding connections. Thanks for quick response!\",\n",
       " \"@SouthwestAir retiring with my hubby and it's our 25th wedding anniversary this year! Companion pass please!!\",\n",
       " '@SouthwestAir sat on a plane for 2+ hrs on 2/15! Then we finally took off.. :(',\n",
       " '@SouthwestAir sent. looking for that functionality in your app, though. should be a one click action.',\n",
       " '@SouthwestAir thanks for the quick response. Should I call daily it wait the 5 days.',\n",
       " \"@SouthwestAir thanks so much just had to make a Cancelled Flightlation! I've sent u the info.\",\n",
       " \"@SouthwestAir thx - i managed to figure it out by trying diff confirmation #s as i booked. just annoying it's not readily available!\",\n",
       " '@SouthwestAir ugh kicking myself for missing out on #destinationdragons. I need some @Imaginedragons back in my life  http://t.co/sbEhi46bMk',\n",
       " \"@SouthwestAir we're here at MCO. Thanks.\",\n",
       " \"@SouthwestAir we're pulling off the runway now! Making new friends with seat mates. Thanks for the response CB.\",\n",
       " '@SouthwestAir when and why was flight 4471 from MSP Cancelled Flighted this AM?',\n",
       " '@SouthwestAir yes. Hung up and called a different number.',\n",
       " \"@SouthwestAir your website has now reverted my 1st flight to only a 5-minute delay. But it's changed, so is that reliable information?\",\n",
       " '@SouthwestAir: grrrrrr I need a flight there first! Maybe for my bday next month...',\n",
       " '@USAirways #555PHLtoSLC this is happening now.',\n",
       " \"@USAirways #ShoutOut 2 Kristie(sp?) from Gate4 @ PVD today. She's a #RockStar, was a tremendous help in a tough situation. #PromoteThatGirl\",\n",
       " '@USAirways 8am flight to mco canld &amp; resch 1:30.If we opt for 8 am depart tom will us air allow cng return day from sat to sun w/o penalty?',\n",
       " '@USAirways @AmericanAir just be prepared for a claim if that bag is lost ... Too many valuables to carry in my hands. #us2146 to #aa2924',\n",
       " \"@USAirways @AmericanAir ok the app doesn't seem to be working. just use the mobile site... http://t.co/4kLfywwMq1\",\n",
       " '@USAirways @VincesViews  there are significant number of folks on 686 connecting to 599 a few minutes hold would make many happy',\n",
       " '@USAirways @husainhaqqani Mr. Husain u shld protest as well when one of ur party member Rehman Malik delayed a PIA flight for hours..???',\n",
       " '@USAirways @united really know how to leave someone stranded after a funeral. Props to you guys. Really making this the perfect Monday.',\n",
       " '@USAirways Cancelled Flightled flight 1796 baggage held hostage at CHL. No communication.',\n",
       " '@USAirways How many agents do you have working to handle the thousands of calls?',\n",
       " '@USAirways I ask for reimbursement maybe miles added to my account for my inconvenience and for the money that I had to spend',\n",
       " '@USAirways I left my personal stuff on US4551 from PHL-BOS about an hour ago. Went baggage, was told plane left to call 800#',\n",
       " '@USAirways I would be willing to pay anything to change a flight! @AllegiantTravel does not! help!',\n",
       " \"@USAirways I'm glad @Beamske retweeted otherwise you would have to ignore the numerous tweets I sent. You all are fucking amazing\",\n",
       " \"@USAirways I'm referring to email like this.  Just tell people when they GET the upgrade. Otherwise, we know already. http://t.co/N8UvCLMADA\",\n",
       " '@USAirways Is there a way to have someone contact me when you have an agent available? Need help Flight Booking Problems an award flight.',\n",
       " '@USAirways Mobile boarding pass email.',\n",
       " '@USAirways Seriously doubt that as I am still sitting inside at the gate.',\n",
       " '@USAirways US 728 Wait, now a gate opened. Back we go. (Good, maybe we can get off.)',\n",
       " \"@USAirways after being assured by the pleasant woman at gate B11 in Albany, NY that life's not perfect, I'd heard enough.\",\n",
       " \"@USAirways after pleading with car service to stay 3 hrs. past pick up they left me and my family but it's ok i have water and crackers!\",\n",
       " '@USAirways been on hold for over and hour now.',\n",
       " '@USAirways can you DM me please?',\n",
       " \"@USAirways doesn't take into account the $450 it cost 2 rent a car 2 drive home\",\n",
       " \"@USAirways doesn't want me to get home\",\n",
       " '@USAirways got off stand by list and on earlier flight to DCA. What is the status of my checked bag? Was supposed to be on #4047 thanks',\n",
       " '@USAirways great but that still does not help me..',\n",
       " \"@USAirways guys I need help my reservations, tried calling and I'm told to call Late Flightr. Please help me here.\",\n",
       " '@USAirways is not the new @AmericanAir is more like the new @SpiritAirlines. Love AA. Not impressed with the subpar planes and gate agents.',\n",
       " \"@USAirways just told by supervisor that they won't guarantee the empty seats to standby passengers with flights Cancelled Flightled so can sell them\",\n",
       " '@USAirways no word on delay reasons @LGB but Twitter is aflame with flight tracking computer crash. Rumors. True or Not?',\n",
       " '@USAirways over the phone. I called the 6170 number and she picked up almost immediately.',\n",
       " '@USAirways please thank Mellie at CAE, Tammy in baggage claim at CLT 4 #excellent customer service 2day, BUT I have a complaint.',\n",
       " '@USAirways res went under her maiden name...married 50...no docs with maiden name.  Hence, problem.',\n",
       " '@USAirways thank you!!!',\n",
       " \"@USAirways that's understandable, my issue is with creating a new flight without the personnel to do it...I changed my plans to accommodate\",\n",
       " \"@USAirways that's who I spoke to already. We've already booked another flight w/someone else. Thanks for your concern. Just wanted to\",\n",
       " \"@USAirways we haven't departed yet so let's not get too high hopes.  But everything has been on schedule so far\",\n",
       " \"@USAirways what's the deal with flight 5268 from DCA to PHL? We boarded on time, left the gate a few mins Late Flight and are now sat on Tarmac?\",\n",
       " \"@USAirways why aren't you updating flight status/delays?\",\n",
       " '@USAirways your saving grace was our flight attendant Dallas who was amazing. wish he would transfer to Delta where I would see him again',\n",
       " '@United are you able to see if there are seats open on another flight??',\n",
       " '@United can you let us out of the gate now. UA1157',\n",
       " '@United never heard of this? http://t.co/QDebyaHqfM',\n",
       " '@United, will you fill it? Yes they will. Thanks! #BringYourOwn, @kleankanteen http://t.co/daaa0rqBXW',\n",
       " '@VirginAmerica\\nwjere is our luggage #so slow at lax',\n",
       " '@VirginAmerica @ladygaga @carrieunderwood - Carrie!',\n",
       " '@VirginAmerica @ladygaga @carrieunderwood Julie Andrews. Hands down.',\n",
       " '@VirginAmerica Atlantic ploughs a lone furrow in the #MiddleEast http://t.co/vw4P4T4tLh @TheNationalUAE',\n",
       " '@VirginAmerica Like http://t.co/VPqEm31XUQ',\n",
       " \"@VirginAmerica You have any flights flying into Boston tomorrow? I need to be home and you Cancelled Flightled my flight and didn't do anything\",\n",
       " '@VirginAmerica always!!! Xoxo',\n",
       " \"@VirginAmerica now it's just t-minus 32 minutes until my Elevate a Silver upgrade window opens . #FreeNeverSucks üòÉüëç\",\n",
       " \"@VirginAmerica seats in Row 8 don't recline should mention that on your website #soreback\",\n",
       " '@americanair Any hints to get through to and stay connected with you to rearrange weather reLate Flightd Cancelled Flights? Hours in now w/nothing.',\n",
       " '@americanair Can you tell me where my luggage is? I was on flight 1644 SNA to DFW and my flight to MKE was Cancelled Flightled, so got rebooked to ORD',\n",
       " '@americanair Help!  I need to speak to a live agent before I lose my online reservation being held.',\n",
       " '@jetblue Vegas desk said JFK connect would b held 4 us and 26 others on our plane now staff at JFK says weather was problem. #epicfail',\n",
       " \"@southwestair TREMENDOUS job. Atlanta Airport saw SW celebrate Mardi Gras. Another reason I'm nuts for you guys! http://t.co/8WBzOrRn3C\",\n",
       " '@southwestair yeah, 7am flight tomorrow, going to add 60-90 min extra because of the roads / snow',\n",
       " '@united  No first class passenger should have to pay for inflight wifi.',\n",
       " '@united  delayed about 8 hours because of missed connections due to mechanical issues on 1st flight. rebooked, but please call me 9148445695',\n",
       " '@united  of course this morning I see a non-stop from IAH to SFO but that was not available yesterday. #UnitedHatesUsAll',\n",
       " \"@united  they were empty upon takeoff. Why can't someone sit there?\",\n",
       " \"@united  why is it so hard to add in meal preferences AFTER I booked tickets? I forgot to do it &amp; it's now impossible! I even called.\",\n",
       " \"@united - you sure missed the mark on tonight's redeye from LAX to Chicago. What a mess! You can do better!\",\n",
       " \"@united 374 ORD to ROC. Fam came to see me at SNA. I'm a member, so is my dad. He used his miles for them.\",\n",
       " '@united Airline trouble this winter &amp; not getting good customer service? contact http://t.co/aQjn4HwNaC  we negotiate resolutions for You!',\n",
       " '@united Bad coffee, but the juice is ok. #GetPhilz @ United Terminal SFO Airport http://t.co/Sg7IQqFVsO',\n",
       " '@united Boeing 777 Star Alliance short finals @Heathrowairport 27L on an amazing winters morning #777 #Avgeek http://t.co/owMaXOyEhZ',\n",
       " \"@united Every United flight between Saipan and Guam is an adventure! You never know when @CapeAir's old plane will be operational :)\",\n",
       " '@united Hopefully my baggage fees will be waived tomorrow when I actually get on a flight, as well as compensation for my hotel room',\n",
       " '@united I hope so too',\n",
       " '@united I need #United to be a better airline!!',\n",
       " '@united I sent you a dm hours ago',\n",
       " \"@united I sent you a dm with my file reference number.. I just want to know if someone has located my bag even if it's not here yet.\",\n",
       " '@united I would like to talk to a customer service agent about the service / non service I received on my last flight a a week or so a go.\\nJ',\n",
       " \"@united I'm flying UA but *G with A3\",\n",
       " \"@united I've sent the message, let me know if you got it. I'm not very twitter-literate. Also, is he REALLY going to be stuck for 18hrs?\",\n",
       " \"@united My post was just more of disappointment.  I'm a frequent United flyer, it was a simple ??.  1 bad apple doesn't spoil the bunch.\",\n",
       " \"@united Not encouraged that I'll have my CPAP tonight.\",\n",
       " '@united Slightly better. Crew spent a long time searching for better altitude. Would have helped to let pax in on why all the sharp drops.',\n",
       " \"@united Well, It's LA and then a 2hr+ car ride to a dark Palm Springs technically. Maybe I'll get bumped to 1st class Den to PHL on Friday:)\",\n",
       " '@united Will you be issuing an exception policy for Denver for this weekend?',\n",
       " '@united Would be helpful if you could refresh boarding time info on boarding passes for delayed flights',\n",
       " \"@united You can bump me up to Group 3 so I won't be forced to check my bag and wait 30+ minutes at LGA.\",\n",
       " '@united You please do not burn my dreams !!!  #burningman tickets sales today.',\n",
       " \"@united a plane took our gate and now we're just waiting in a lot at DIA\",\n",
       " '@united a report was filed with the airport police on 11th and 12th February-I have the police case number if required',\n",
       " \"@united according to your DMs, I'm not owed a refund. please may I be provided with a contact number before I go to my bank to file claim\",\n",
       " '@united add wifi, entertainment and the old seats and i‚Äôll come back.',\n",
       " '@united as for volunteers to give up seats, people did! Now we sit for 25 minutes on plane waiting 4 them to add more people!  #letsgo',\n",
       " '@united blood services, make your appt. today! 18009174929 http://t.co/6UXwPaduGS',\n",
       " '@united can you get a gate for UA4727?  Turrible.',\n",
       " '@united done as requested.',\n",
       " '@united flight 5187 to be specific. The last two were probably 30 feet apart and within sight of each other',\n",
       " '@united flight arrives 30 minutes early, but then have we to wait for an hour for our bags.',\n",
       " '@united have better customer service at John Wayne airport.',\n",
       " '@united have reported it.  Still in Istanbul at the moment apparently.  On the other plane haha.  Hats off to the pilot!',\n",
       " '@united hi I left my asus tablet on flight to Heathrow from Newark on 16th feb and wondered how long it takes for someone to check if found?',\n",
       " '@united kind of unnerving to watch the guy deicing your plane text on his phone the whole time http://t.co/QdXfT9qqT9',\n",
       " '@united no it weighed 45.5 and it was the only checked bag',\n",
       " '@united no worries about the tweets. We all should do what we can to make sure we, as in your tag line, \"Fly the Friendly Skies.\"',\n",
       " '@united nope. Really. No window at all for 10A. 10F has one. 8 and 11 has one. None for 10A. So sad...',\n",
       " '@united on plane to Newark now. I was checked in as Rebecca Levi. My bag is under her name too.',\n",
       " \"@united please tell me I'm going to make my connecting flight from O'hare to #STL #SouthBendINWhere üôè http://t.co/qGwK10dEwv\",\n",
       " '@united right... Are you guys charging for the air we breathe next?',\n",
       " \"@united same flight number different flight. I'm heading to MPLS\",\n",
       " '@united seriously. This is just complete bs.',\n",
       " '@united so our flight into ORD was delayed because of Air Force One, but the last flight to SBN is at 8:20, 5 mins from now we just landed.',\n",
       " '@united thank you for the help!!',\n",
       " '@united thank you for the quick response but I cannot dm you until you follow me as well',\n",
       " \"@united thank you.\\nIt's my daughters 13th bd party w/proj. weather cond, it doesn't look promising.\\nPlease assist with earlier flts to Cmh?\",\n",
       " '@united thanks',\n",
       " '@united thanks ^mr i got rebooked already but I lost my first class seat. Such is life.',\n",
       " '@united thanks for having changed me. Managed to arrive with only 8 hours of delay and exhausted',\n",
       " '@united the wifi in the ewr lounge reminds me of the old days of dial up',\n",
       " '@united this is in ur Hemispheres magazine. I\\'m open to what will u do to make the \"flight more pleasant.\" http://t.co/VrQDpqEPFW',\n",
       " '@united went to Customer Service kiosk and they were able to help out! Still thanks for following up!',\n",
       " '@united will I make it with the delay?',\n",
       " '@united yep‚Äîused that twice; no case id numbers assigned; just tried it again-get screen that says, ‚Äúthank you for submitting your request‚Äù',\n",
       " '@united yes I do.',\n",
       " \"@united yes I've boarded this way many times &amp; have never had to show my pass on the Tarmac multiple times. Path was railed off. Only 1 way\",\n",
       " '@united yes, after landing at 930pm last night. Spoke to 10 baggage claim customer service reps. Finally spoke to one amazing ticket agent.',\n",
       " '@united yo, you need a new website really badly. This day and age...',\n",
       " '@united you all do a wonderful job today. Got my wife, daughter, and myself from PGH to Orlando after out flight was delayed luggage and all',\n",
       " '@united you changed my entire flight plan for vacation and now I will be there alone a day and night early with nowhere to stay. Help!',\n",
       " '@united you need a bag bouncer. Get it together',\n",
       " \"@united your website won't allow me to post the required document, i keep getting **were having technical difficulties**\",\n",
       " \"@usairways Thanks so much for ruining my vacation. I really couldn't have done it without you.\",\n",
       " 'Flight Booking Problems for a travel writers trip I am hosting and need to be happy when I land! Do you have the cute PJs AA does? @united',\n",
       " \"RT @JetBlue: Our fleet's on fleek. http://t.co/NfjAuW16vZ&lt;--....üòï\",\n",
       " 'Stop the madness \"@JetBlue: Our fleet\\'s on fleek. http://t.co/Q5a7jtkI5K‚Äù',\n",
       " 'Thank you @united for your prompt assistance.',\n",
       " \"Thanks! Hope I don't look like a complainer, I've written of good experiences w/you in the past and am making do in terminal 5 :) @JetBlue\",\n",
       " \"Why? üòí RT @JetBlue Our fleet's on fleek. http://t.co/I7Ut2ZvHCO\",\n",
       " nan,\n",
       " nan,\n",
       " '‚Äú@AmericanAir: @nlrphoto Those are very beautiful photos!‚Äù Thx! Would marketing dept be interested in buying?',\n",
       " \"‚Äú@AmericanAir: Bet these birds wish they'd flown south for the #winter... http://t.co/tY9C0Gae2o‚Äù Lol\",\n",
       " \"‚Äú@JetBlue: Our fleet's on fleek. http://t.co/3Ltx7JKBo9‚Äù is fleek dead yet now?\",\n",
       " \"‚Äú@JetBlue: Our fleet's on fleek. http://t.co/4LlWi5oxvO‚Äù lmfaooooo hook me up with a flight!\",\n",
       " \"‚Äú@JetBlue: Our fleet's on fleek. http://t.co/M2wSg2olgo‚Äù -_-\",\n",
       " \"‚Äú@JetBlue: Our fleet's on fleek. http://t.co/we7Pf5Ll1Y‚Äù lol!!!!!\",\n",
       " '‚Äú@SouthwestAir: @kirkwoodtiger Hmmm... how does the Caribbean sound? https://t.co/AAY5avg99b ^LD‚Äù WARM THANKS!'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter1 = df['airline_sentiment_confidence'] > 0.7\n",
    "filter2 =  df['airline_sentiment_confidence'] < 0.9\n",
    "df_1 = df.where(filter1 )\n",
    "df_1 = df_1.where(filter2 )\n",
    "set(df_1['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10445\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.where(df['airline_sentiment_confidence'] == 1)\n",
    "#df_clean = df_clean.dropna()\n",
    "texts = df_clean['text'].dropna().tolist()\n",
    "labels = df_clean['airline_sentiment'].dropna().tolist()\n",
    "assert(len(texts) == len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10445\n"
     ]
    }
   ],
   "source": [
    "txts = []\n",
    "for i in texts:\n",
    "    #print(type(i), i)\n",
    "    all_x = re.findall('[@][a-z]*', i, re.I)\n",
    "    if all_x is not None:\n",
    "        for x in all_x:\n",
    "            i = i.replace(x, '')\n",
    "    txts.append(i)\n",
    "texts = txts\n",
    "assert(len(texts) == len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "\n",
    "class ClassificationLinearModel(nn.Module):\n",
    "    def __init__(self, in_feat=1024, out_class=27, num_lin_lrs=None,\n",
    "                 dropout=0.0, gpu=-1, activation='TANH', is_debug=False):\n",
    "        super(ClassificationLinearModel, self).__init__()\n",
    "\n",
    "        self.gpu = gpu\n",
    "        self.is_debug = is_debug\n",
    "        self.layers = num_lin_lrs\n",
    "        self.input_nodes = in_feat\n",
    "        self.output_nodes = out_class\n",
    "        self.dropout = dropout\n",
    "        modules = []\n",
    "        # create a MLP with given layers (list of values)\n",
    "        if num_lin_lrs is not None and len(num_lin_lrs) > 0:\n",
    "            # size of previous layer in NN\n",
    "            nprev = in_feat\n",
    "            for nh in num_lin_lrs:  # number of hidden nodes in the configuration\n",
    "                if nh > 0:\n",
    "                    modules.append(nn.Linear(nprev, nh))\n",
    "                    nprev = nh\n",
    "                    # after each linear layer add activation function\n",
    "                    if activation == 'TANH':\n",
    "                        modules.append(nn.Tanh())\n",
    "                        print('-{:d}t'.format(nh), end='')\n",
    "                    elif activation == 'RELU':\n",
    "                        modules.append(nn.ReLU())\n",
    "                        print('-{:d}r'.format(nh), end='')\n",
    "                    else:\n",
    "                        raise Exception('Unrecognized activation {activation}')\n",
    "                    # after each activation add dropout if specified\n",
    "                    if dropout > 0:\n",
    "                        modules.append(nn.Dropout(p=dropout))\n",
    "            # add output layer\n",
    "            modules.append(nn.Linear(nprev, out_class))\n",
    "            print('-{:d}, dropout={:.1f}'.format(out_class, dropout))\n",
    "        else:  # shallow NN\n",
    "            # add dropuot layer\n",
    "            if dropout > 0:\n",
    "                modules.append(nn.Dropout(p=dropout))\n",
    "            # add output layer\n",
    "            modules.append(nn.Linear(in_feat, out_class))\n",
    "            print(' - mlp %d-%d'.format(in_feat, out_class))\n",
    "        # add all layers to the sequential model\n",
    "        self.mlp = nn.Sequential(*modules)\n",
    "        # optimize for GPU\n",
    "        if self.gpu >= 0:\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            self.mlp = self.mlp.to(self.device)  # cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_debug:\n",
    "            logging.debug(\"The shape of x is {}\".format(x.shape))\n",
    "            print(\"The shape of x is {}\".format(x.shape))\n",
    "        # optimize for GPU\n",
    "        if self.gpu >= 0:\n",
    "            x = x.to(self.device)\n",
    "        ret = self.mlp(x)\n",
    "        if self.is_debug:\n",
    "            logging.warning(\"The shape after forward pass is {}\".format(ret.shape))\n",
    "            print(\"The shape after forward pass is {}\".format(ret.shape))\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def save(self, filepath, class_to_numb, numb_to_class):\n",
    "        \"\"\"Save the model to a file.\n",
    "            Args:\n",
    "                        filepath: the path of the file.\n",
    "            \"\"\"\n",
    "        torch.save({\n",
    "             'state_dict': self.state_dict(),\n",
    "             'num_lin_lrs': self.layers,\n",
    "             'out_class': self.output_nodes,\n",
    "             'dropout': self.dropout,\n",
    "             'class_to_numb': class_to_numb,\n",
    "             'numb_to_class': numb_to_class\n",
    "         }, filepath)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Load the model from a file.\n",
    "            Args:\n",
    "                      filepath: the path of the file.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location={'cuda:0': 'cpu'})\n",
    "        class_to_numb = checkpoint['class_to_numb']\n",
    "        numb_to_class = checkpoint['numb_to_class']\n",
    "        num_lin_lrs = checkpoint['num_lin_lrs']\n",
    "        out_classes = checkpoint['out_class']\n",
    "        dropout = checkpoint['dropout']\n",
    "        obj = EmailPartDetectorLinearModel(out_class=out_classes, num_lin_lrs=num_lin_lrs, dropout=dropout)\n",
    "        obj.load_state_dict(checkpoint['state_dict'])\n",
    "        return obj, class_to_numb, numb_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of correct predictions (tensor)\n",
    "def get_correct_predictions(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "\n",
    "# retrieve vector for the sentence - LASER\n",
    "def retrieve_embeddings(embed_service, sentences, lang='en'):\n",
    "    urlstr = embed_service.strip('/') + '/embed'\n",
    "    urlstr = urlstr + '/' + lang\n",
    "    input_text  = {'sentences':sentences}\n",
    "    response = requests.post(urlstr, json=input_text).json()\n",
    "    base_embs = response['embedding']\n",
    "    return base_embs\n",
    "\n",
    "\n",
    "# Converts classes to sequential numbers - the model needs numbers as classes\n",
    "# Returns two dictionaries - classes to numbers and numbers to classes\n",
    "def create_classes_to_numbers_convertion(list_classes):\n",
    "    possible_classes = list(sorted(set(list_classes)))  # make a sorted list of all possible classes\n",
    "    # print('possibleClasses \\t', possibleClasses)\n",
    "    class_to_numb = dict()\n",
    "    numb_to_class = dict()\n",
    "    for i in range(len(possible_classes)):\n",
    "        class_to_numb[possible_classes[i]] = i\n",
    "        numb_to_class[i] = possible_classes[i]\n",
    "    return class_to_numb, numb_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e5be47e40595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclass_to_numb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumb_to_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_classes_to_numbers_convertion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# convert list to numpy array for further conversion to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnbex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mdim\u001b[0m  \u001b[0;31m# number of sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "dim = 1024\n",
    "\n",
    "embedding_laser = retrieve_embeddings('http://localhost:7005', texts)\n",
    "\n",
    "class_to_numb, numb_to_class = create_classes_to_numbers_convertion(labels)\n",
    "# convert list to numpy array for further conversion to tensor\n",
    "embeddings = np.array(embeddings, dtype='float32')\n",
    "\n",
    "nbex = embeddings.shape[0] // dim  # number of sentences\n",
    "# to reshape to correct size     #embedding = embedding.reshape(nbex,dim)\n",
    "embeddings.resize(nbex, dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# convert list to numpy array for further conversion to tensor\n",
    "embeddings = np.array(embedding_laser, dtype='float32')\n",
    "\n",
    "nbex = embeddings.shape[0] // dim  # number of sentences\n",
    "# to reshape to correct size     #embedding = embedding.reshape(nbex,dim)\n",
    "embeddings.resize(nbex, dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10445, 1024)\n",
      "10445 1\n"
     ]
    }
   ],
   "source": [
    "numbers = [class_to_numb[i] for i in labels]\n",
    "print(embeddings.shape)\n",
    "print(len(numbers), numbers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as torchoptimizer\n",
    "import torch.nn.functional as func\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# train PyTorch NN model\n",
    "def train_model(lin_lrs, embeddings, classes, dropout=0.0,\n",
    "                numb_of_epochs=25, lr=0.01, gpu=-1):\n",
    "\n",
    "    logging.debug(\"Possible Classes set size: {}\".format(len(list(set(classes)))))\n",
    "    # train model\n",
    "    our_model = ClassificationLinearModel(\n",
    "        num_lin_lrs=lin_lrs, out_class=len(list(set(classes))), gpu=gpu, dropout=dropout)\n",
    "    optimizer = torchoptimizer.Adam(our_model.parameters(), lr=lr)\n",
    "\n",
    "    # TensorBoard initialization\n",
    "    # comment = f'number of epochs={numbOfEpochs}, layers={lin_lyrs}, dropout={dropout}, learning_rate={lr}'\n",
    "    # tb = SummaryWriter(comment=comment)\n",
    "    # logging data\n",
    "    logging.warning(\"Embedding length: {}, classes length {}\".format(len(embeddings), len(classes)))\n",
    "    # divide it to train and validation sets\n",
    "    train_emb, val_emb, train_clss, val_clss = train_test_split(embeddings, classes,\n",
    "                                                                test_size=0.1, random_state=12)\n",
    "    # preparing for validation\n",
    "    best_loss = 0.0\n",
    "    best_model = None\n",
    "    # prepare the dataset\n",
    "    train_tensors = torch.as_tensor(train_emb)\n",
    "    # make tansor from labels\n",
    "    label_tensors = torch.from_numpy(np.array(train_clss, dtype='int64'))\n",
    "    print('shapes', train_tensors.shape, len(train_clss))\n",
    "    # for training in batches - make a dataset from 2 tensors (embeddings and label)\n",
    "    united_dataset = torch.utils.data.TensorDataset(train_tensors, label_tensors)\n",
    "    # load in batches of 54 (27 classes * 2)\n",
    "    # do the shuffling for each epoch to better train the model\n",
    "    train_loader = torch.utils.data.DataLoader(united_dataset,\n",
    "                                               batch_size=5 * len(list(set(classes))),\n",
    "                                               shuffle=True)\n",
    "    if gpu >= 0:  # move everuthing to GPU\n",
    "        device = torch.device(\"cuda\")\n",
    "        our_model.to(device)\n",
    "        optimizer.to(device)\n",
    "\n",
    "    # run training\n",
    "    for epoch in range(numb_of_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        # # training for one epoch\n",
    "        # criterion = nn.CrossEntropyLoss() #### another way of calculating loss\n",
    "        for batch in train_loader:\n",
    "            sntncs, lbls = batch\n",
    "            if gpu >= 0:  # move everuthing to GPU\n",
    "                device = torch.device(\"cuda\")\n",
    "                sntncs = sntncs.to(device)\n",
    "                lbls = lbls.to(device)\n",
    "\n",
    "            preds = our_model(sntncs)\n",
    "            loss = func.cross_entropy(preds, lbls)\n",
    "            # zero all previous values and do the learnimng\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # calculate loss and correct preds during the training\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_correct_predictions(preds, lbls)\n",
    "\n",
    "        '''#### TensorBoard \n",
    "        tb.add_scalar('Loss', total_loss, epoch)\n",
    "        tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "        tb.add_scalar('Accuracy', (total_loss/len(shuffledLbls)), epoch)\n",
    "\n",
    "        for name, weight in ourModel.named_parameters():\n",
    "            tb.add_histogram(name, weight, epoch)\n",
    "            tb.add_histogram(f'{name}.grad', weight.grad, epoch)'''\n",
    "\n",
    "        logging.debug(\"Epoch {}, total_correct {}, total_loss {}\".format(\n",
    "            epoch, total_correct, total_loss))\n",
    "\n",
    "        # check it against validation loss\n",
    "        # save the best model with validation loss\n",
    "        our_model.eval()\n",
    "        valid_tensors = torch.as_tensor(val_emb)\n",
    "        valid_labels = torch.from_numpy(np.array(val_clss, dtype='int64'))\n",
    "        if gpu >= 0:\n",
    "            device = torch.device(\"cuda\")\n",
    "            valid_tensors = train_tensors.to(device)\n",
    "            valid_labels = label_tensors.to(device)\n",
    "        valid_preds = our_model(valid_tensors)\n",
    "        valid_loss = func.cross_entropy(valid_preds, valid_labels)\n",
    "        # assign best loss value != 0 for the first time\n",
    "        if best_loss == 0:  # for the first time\n",
    "            best_loss = valid_loss + 1\n",
    "        # check the best model\n",
    "        if valid_loss < best_loss:\n",
    "            logging.warning(\n",
    "                'The best model according to validation. Loss: {}, epoch: {}, Valid Loss: {}'.format(\n",
    "                    total_loss, epoch, valid_loss))\n",
    "            best_loss = valid_loss\n",
    "            best_model = copy.deepcopy(our_model)\n",
    "        our_model.train(True)\n",
    "        # end of validation\n",
    "\n",
    "    return our_model, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# divide it to train and validation sets\n",
    "train_emb, test_emb, train_clss, test_clss = train_test_split(embeddings, numbers, test_size=0.1, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb, val_emb, train_clss, val_clss = train_test_split(train_emb, train_clss, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One model for all classes\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "layers = [[700], [500], [300],  # [351], [700], [540],\n",
    "                  [700, 300], [500, 100],  # [648, 324], [540, 64],\n",
    "                  # [540, 256, 64], [784, 356, 70], [720, 326, 54], [783, 405, 135],\n",
    "                  # [720, 500, 280, 81]\n",
    "                  ]\n",
    "dropouts = [0.0, 0.1, 0.2]  # , 0.3]\n",
    "epochs = [40, 60, 80]  # 70,50,30,\n",
    "lrates = [0.01, 0.001]  # 0.1,\n",
    "        # build parameters:\n",
    "parameters = dict(\n",
    "    layers=layers,\n",
    "    dropouts=dropouts,\n",
    "    epochs=epochs,\n",
    "    lrates=lrates\n",
    "    )\n",
    "param_values = [v for v in parameters.values()]\n",
    "\n",
    "print(\"One model for all classes\")\n",
    "best_result = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "def test_model(our_model, test_embedding, testclasses, gpu=-1):\n",
    "    # make tensor from labels/classes\n",
    "    test_lbls = torch.from_numpy(np.array(testclasses, dtype='int64'))\n",
    "    test_nbex_len = len(testclasses)  # total number of questions\n",
    "    test_tensors = torch.as_tensor(test_embedding)\n",
    "    if gpu >= 0:\n",
    "        device = torch.device(\"cuda\")\n",
    "        test_tensors = test_tensors.to(device)\n",
    "        test_lbls = test_lbls.to(device)\n",
    "        our_model = our_model.to(device)\n",
    "    # get predictions\n",
    "    our_model.eval()  # otherwise it will go into learning mode\n",
    "    test_preds = our_model(test_tensors)\n",
    "    # get correct predictions\n",
    "    test_total_correct = get_correct_predictions(test_preds, test_lbls)\n",
    "    logging.debug(\"testpreds {}\\ntest_lbls {}\".format(\n",
    "        test_preds.argmax(dim=1), test_lbls))\n",
    "    logging.debug(\n",
    "        \"TEST: total {}, total_correct {}, accuracy {}\".format(\n",
    "            test_total_correct, test_nbex_len,\n",
    "            (test_total_correct / test_nbex_len)\n",
    "        )\n",
    "    )\n",
    "    return test_total_correct, test_nbex_len, (test_total_correct / test_nbex_len), \\\n",
    "           test_lbls.cpu().detach().numpy(), (test_preds.argmax(dim=1)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Embedding length: 8460, classes length 8460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-700t-3, dropout=0.0\n",
      "shapes torch.Size([7614, 1024]) 7614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The best model according to validation. Loss: 217.60114764422178, epoch: 0, Valid Loss: 0.31941041350364685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-b863015859d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     the_model, best_val_model = train_model(\n\u001b[1;32m      9\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_clss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         lr=lr, numb_of_epochs=ep, gpu=-1)  # gpu=0 if is_cuda else -1) #\n\u001b[0m\u001b[1;32m     11\u001b[0m     test_correct, test_total, result, y_true, y_pred = test_model(\n\u001b[1;32m     12\u001b[0m         \u001b[0mbest_val_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbest_val_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mthe_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_clss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-91816b242929>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(lin_lrs, embeddings, classes, dropout, numb_of_epochs, lr, gpu)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m# calculate loss and correct preds during the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "\n",
    "\n",
    "# do the experiments for all the parameters\n",
    "for layer, drp, ep, lr in product(*param_values):\n",
    "    the_model, best_val_model = train_model(\n",
    "        layer, train_emb, train_clss, dropout=drp,\n",
    "        lr=lr, numb_of_epochs=ep, gpu=-1)  # gpu=0 if is_cuda else -1) #\n",
    "    test_correct, test_total, result, y_true, y_pred = test_model(\n",
    "        best_val_model if best_val_model is not None else the_model, test_emb, test_clss,\n",
    "        gpu=-1)\n",
    "    if best_val_model is None:\n",
    "        logging.warning('Validation model is None----------------')\n",
    "    logger.debug('Results: {}, {}, {}, {}, {}, {}, {};'.format(layer, drp, ep, lr, test_correct,\n",
    "                                                              test_total, result))\n",
    "    '''helper.save_result_to_file(\n",
    "        '.', layer, drp, ep, lr, test_correct, test_total, result)'''\n",
    "    # get the best model\n",
    "    if result >= best_result:\n",
    "        logging.debug(\n",
    "            '''Selected best model with accuracy {}. \n",
    "            Parameters: layers {}, dropout {}, num_of_epochs {}, learn_rate {}'''.format(\n",
    "                result, layer, drp, ep, lr))\n",
    "        best_result = result\n",
    "        # make theModel the best validated model. Then save it\n",
    "        the_model = best_val_model if best_val_model is not None else the_model\n",
    "        best_model = copy.deepcopy(the_model)\n",
    "        # if the file exists - delete it. then save the state of the model\n",
    "        modelfile = './bestmodel_{}.pth'.format(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "        if os.path.exists(modelfile):\n",
    "            os.remove(modelfile)\n",
    "                # when saving - save everything to be able to reload the model\n",
    "        torch.save({\n",
    "            'state_dict': best_model.state_dict(),\n",
    "            'num_lin_lrs': layer,\n",
    "            'out_class': len(list(set(classes_all))),\n",
    "            'dropout': drp,\n",
    "            'class_to_numb': class_to_numb,\n",
    "            'numb_to_class': numb_to_class\n",
    "        }, modelfile)\n",
    "                # now save the confusion matrix\n",
    "        class_names = np.asarray([i for i in numb_to_class.values()], dtype=np.str)\n",
    "        \n",
    "                # Print the precision and recall, among other metrics\n",
    "        print(classification_report(y_true, y_pred, digits=3))\n",
    "        logging.debug(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
